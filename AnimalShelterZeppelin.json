{"paragraphs":[{"text":"// Read the CSV file.\nval pathToInputFile = \"/Users/harihar/AnimalData/train.csv\"\nval trainingDf = sqlContext.read\n    .format(\"com.databricks.spark.csv\")\n    .option(\"header\", \"true\")\n    .option(\"inferSchema\", \"true\")\n    .load(pathToInputFile)","authenticationInfo":{},"dateUpdated":"May 9, 2016 9:29:41 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1462820519206_-459913435","id":"20160509-150159_1067574983","dateCreated":"May 9, 2016 3:01:59 PM","dateStarted":"May 9, 2016 9:29:41 PM","dateFinished":"May 9, 2016 9:29:42 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:128","errorMessage":"","focus":true},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1462834444720_-385266471","id":"20160509-185404_1431714063","dateCreated":"May 9, 2016 6:54:04 PM","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:562","text":"// Create a helper function that can turn the age string into the number of days.\ndef ageToDays(age: String): Double = {\n    if (age.isEmpty) {\n        0\n    } else {\n        val split = age.split(\" \")\n        val num = split.head.toDouble\n        val span = split.tail.head.toString\n        val multiplier: Double = span match {\n            case \"years\" => 365.25\n            case \"year\" => 365.25\n            case \"days\" => 1\n            case \"day\" => 1\n            case \"month\" => 30\n            case \"months\" => 30\n            case \"week\" => 7\n            case \"weeks\" => 7\n        }\n        multiplier * num\n    }\n}","dateUpdated":"May 9, 2016 9:29:41 PM","dateFinished":"May 9, 2016 9:29:42 PM","dateStarted":"May 9, 2016 9:29:41 PM","errorMessage":""},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1462841966425_-501085601","id":"20160509-205926_566541994","dateCreated":"May 9, 2016 8:59:26 PM","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1685","text":"// Convert the timestamp into a Long.\nimport org.apache.spark.sql.Row\nimport org.apache.spark.sql.types.{LongType, StructType}\nimport java.text.SimpleDateFormat\n\n// First create a new schema where the age field is not a String, but is instead a Double.\nval formatter = new SimpleDateFormat(\"yyyy-mm-dd hh:ss:mm.M\")\nval newSchema = trainingDf.schema.zipWithIndex.map { case (field, index) =>\n    if (index == 2) {\n        field.copy(dataType=LongType)\n    } else {\n        field\n    }\n}\n\n// Iterate through each row and replace the age string with the age in days.\nval trainingDfWithLong = sqlContext.createDataFrame(trainingDf.map { row =>\n    Row.fromSeq(row.toSeq.zipWithIndex.map { case (item, index) =>\n        if (index == 2) {\n            formatter.parse(item.toString).getTime\n        } else {\n            item\n        }\n    })\n}, StructType(newSchema))","dateUpdated":"May 9, 2016 9:29:41 PM","dateFinished":"May 9, 2016 9:29:44 PM","dateStarted":"May 9, 2016 9:29:42 PM","errorMessage":""},{"text":"// Convert the age into a double.\nimport org.apache.spark.sql.Row\nimport org.apache.spark.sql.types.{DoubleType, StructType}\n\n// First create a new schema where the age field is not a String, but is instead a Double.\nval newSchema = trainingDfWithLong.schema.zipWithIndex.map { case (field, index) =>\n    if (index == 7) {\n        field.copy(dataType=DoubleType)\n    } else {\n        field\n    }\n}\n\n// Iterate through each row and replace the age string with the age in days.\nval trainingDfWithDaysCol = sqlContext.createDataFrame(trainingDfWithLong.map { row =>\n    Row.fromSeq(row.toSeq.zipWithIndex.map { case (item, index) =>\n        if (index == 7) {\n            ageToDays(item.toString)\n        } else {\n            item\n        }\n    })\n}, StructType(newSchema))","authenticationInfo":{},"dateUpdated":"May 9, 2016 9:29:41 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1462820601532_1027464516","id":"20160509-150321_1612722870","dateCreated":"May 9, 2016 3:03:21 PM","dateStarted":"May 9, 2016 9:29:43 PM","dateFinished":"May 9, 2016 9:29:46 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:129","errorMessage":"","focus":true},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1462820904014_-607103698","id":"20160509-150824_669780109","dateCreated":"May 9, 2016 3:08:24 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:132","dateUpdated":"May 9, 2016 9:29:41 PM","dateFinished":"May 9, 2016 9:29:47 PM","dateStarted":"May 9, 2016 9:29:45 PM","errorMessage":"","text":"// Create a preprocessing pipeline for the dataset based on the desired columns.\nimport org.apache.spark.ml.Pipeline\nimport org.apache.spark.ml.feature.{StringIndexer, VectorIndexer, VectorAssembler}\n\ndef makePreprocessingPipeline(desiredColumns: Array[String]): Pipeline = {\n    // Remove the ID, because that's probably not predictive, and remove the OutcomeType because that's what I\n    // want to predict. Also, OutcomeSubtype should be removed too.\n    val actualDesiredColumns = desiredColumns.diff(Array(\"AnimalID\", \"OutcomeType\", \"OutcomeSubtype\"))\n    \n    // Index the label.\n    val labelIndexer = new StringIndexer()\n        .setInputCol(\"OutcomeType\")\n        .setOutputCol(\"label\")\n    \n    // Create string indexers for each column.\n    val indexers = actualDesiredColumns.map { col =>\n        new StringIndexer()\n            .setInputCol(col)\n            .setOutputCol(col + \"_idx\")\n    }\n    \n    // Assemble the vectors.\n    val assembler = new VectorAssembler()\n        .setInputCols(actualDesiredColumns.map(_ + \"_idx\"))\n        .setOutputCol(\"features\")\n    \n    // Mark nominal variables (anything with < 100 categories).\n    val vectorIndexer = new VectorIndexer()\n        .setInputCol(\"features\")\n        .setMaxCategories(100)\n    \n    new Pipeline()\n        .setStages(Array(labelIndexer) ++ indexers ++ Array(assembler, vectorIndexer))\n}","focus":true},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1462835928264_2063757681","id":"20160509-191848_1786017572","dateCreated":"May 9, 2016 7:18:48 PM","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:629","dateUpdated":"May 9, 2016 9:29:41 PM","dateFinished":"May 9, 2016 9:29:47 PM","dateStarted":"May 9, 2016 9:29:46 PM","errorMessage":"","text":"// Create a function that will create a decision tree classifier.\nimport org.apache.spark.ml.classification.DecisionTreeClassifier\n\ndef makeDecisionTree(): DecisionTreeClassifier = {\n    new DecisionTreeClassifier()\n        .setFeaturesCol(\"features\")\n        .setPredictionCol(\"prediction\")\n        .setLabelCol(\"label\")\n        .setMaxBins(1500)\n}"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1462835998382_890581663","id":"20160509-191958_673541175","dateCreated":"May 9, 2016 7:19:58 PM","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:709","dateUpdated":"May 9, 2016 9:29:41 PM","dateFinished":"May 9, 2016 9:29:48 PM","dateStarted":"May 9, 2016 9:29:47 PM","errorMessage":"","text":"// Make a function that will evaluate a set of predictions.\nimport org.apache.spark.sql.DataFrame\nimport org.apache.spark.mllib.evaluation.MulticlassMetrics \n\n\ndef evaluate(df: DataFrame): Unit = {\n    val pairs = df.select(df(\"prediction\"), df(\"label\")).map { row =>\n        (row(0).asInstanceOf[Double], row(1).asInstanceOf[Double])\n    }\n    val metrics = new MulticlassMetrics(pairs)\n    println(metrics.fMeasure)\n    println(metrics.precision)\n    println(metrics.recall)\n    println(metrics.confusionMatrix)\n}"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1462836432730_-1794968948","id":"20160509-192712_1410766747","dateCreated":"May 9, 2016 7:27:12 PM","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:933","text":"// Make a function to create a train/test split.\ndef split(df: DataFrame): (DataFrame, DataFrame) = {\n    val splitFrames = df.randomSplit(Array(0.7, 0.3))\n    (splitFrames(0), splitFrames(1))\n}","dateUpdated":"May 9, 2016 9:29:41 PM","dateFinished":"May 9, 2016 9:29:49 PM","dateStarted":"May 9, 2016 9:29:48 PM","errorMessage":""},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1462836358652_1999181162","id":"20160509-192558_218655976","dateCreated":"May 9, 2016 7:25:58 PM","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:797","dateUpdated":"May 9, 2016 9:29:41 PM","dateFinished":"May 9, 2016 9:29:49 PM","dateStarted":"May 9, 2016 9:29:49 PM","errorMessage":"","text":"// Make a function that combines the above to train and evaluate a decision tree.\ndef fullDecisionTree(cols: Array[String]): Unit = {\n    val pipeline = makePreprocessingPipeline(cols)\n    val pipelineModel = pipeline.fit(trainingDfWithDaysCol)\n\n    // Print out the labels.\n    import org.apache.spark.ml.feature.StringIndexerModel\n    pipelineModel.stages(0).asInstanceOf[StringIndexerModel].labels\n    \n    val preprocessedData = pipelineModel.transform(trainingDfWithDaysCol)\n    \n    val (train, test) = split(preprocessedData)\n    \n    val dt = makeDecisionTree\n    val dtModel = dt.fit(train)\n    val predictions = dtModel.transform(test)\n    \n    evaluate(predictions)\n    println(dtModel.toDebugString)\n}"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1462836401204_-1172875949","id":"20160509-192641_367390764","dateCreated":"May 9, 2016 7:26:41 PM","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:865","dateUpdated":"May 9, 2016 9:29:41 PM","dateFinished":"May 9, 2016 9:29:54 PM","dateStarted":"May 9, 2016 9:29:49 PM","errorMessage":"","text":"// I'll first only use the SexuponOutcome columns.\nfullDecisionTree(Array(\"SexuponOutcome\"))"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1462836860768_-745712725","id":"20160509-193420_1888328751","dateCreated":"May 9, 2016 7:34:20 PM","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1190","dateUpdated":"May 9, 2016 9:29:41 PM","dateFinished":"May 9, 2016 9:29:59 PM","dateStarted":"May 9, 2016 9:29:50 PM","errorMessage":"","text":"// The above model only predicts transferred or adopted. The F-measure is about 0.6, which is above chance.\n// Now I'll try adding in the AgeuponOutcome, because I think that could be predictive too.\nfullDecisionTree(Array(\"SexuponOutcome\", \"AgeuponOutcome\"))\n"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1462837185259_1194220757","id":"20160509-193945_2056125479","dateCreated":"May 9, 2016 7:39:45 PM","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1311","dateUpdated":"May 9, 2016 9:29:41 PM","dateFinished":"May 9, 2016 9:30:07 PM","dateStarted":"May 9, 2016 9:29:54 PM","errorMessage":"","text":"// Using both of the features now makes the model predict 4 of the 5 labels, but the F measure hasn't improved much.\n// I'll add the AnimalType now (e.g. Cat or Dog) because that is also predictive, according to the visualizations.\nfullDecisionTree(Array(\"SexuponOutcome\", \"AgeuponOutcome\", \"AnimalType\"))"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1462837391727_1705291534","id":"20160509-194311_1823464108","dateCreated":"May 9, 2016 7:43:11 PM","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1391","dateUpdated":"May 9, 2016 9:29:41 PM","dateFinished":"May 9, 2016 9:30:07 PM","dateStarted":"May 9, 2016 9:30:00 PM","errorMessage":"","text":"// Again, minor improvement to the F measure, but I'm still not predicting the last label, even though\n// there are 74 animals in my particular test set in that category. Maybe I should use a more powerful model?\n// I'll replace the decision tree with a random forest.\n\nimport org.apache.spark.ml.classification.RandomForestClassifier\ndef fullRandomForest(cols: Array[String]): Unit = {\n    val pipeline = makePreprocessingPipeline(cols)\n    val pipelineModel = pipeline.fit(trainingDfWithDaysCol)\n\n    // Print out the labels.\n    import org.apache.spark.ml.feature.StringIndexerModel\n    pipelineModel.stages(0).asInstanceOf[StringIndexerModel].labels\n    \n    val preprocessedData = pipelineModel.transform(trainingDfWithDaysCol)\n    \n    val (train, test) = split(preprocessedData)\n    \n    val rf = new RandomForestClassifier()\n        .setFeaturesCol(\"features\")\n        .setLabelCol(\"label\")\n        .setPredictionCol(\"prediction\")\n        .setMaxBins(100)\n    val rfModel = rf.fit(train)\n    val predictions = rfModel.transform(test)\n    \n    evaluate(predictions)\n}"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1462837573264_-1584483417","id":"20160509-194613_1699409343","dateCreated":"May 9, 2016 7:46:13 PM","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1459","dateUpdated":"May 9, 2016 9:29:41 PM","dateFinished":"May 9, 2016 9:30:32 PM","dateStarted":"May 9, 2016 9:30:07 PM","errorMessage":"","text":"// Try with the three feature sets I used before.\nfullRandomForest(Array(\"SexuponOutcome\"))\nfullRandomForest(Array(\"SexuponOutcome\", \"AgeuponOutcome\"))\nfullRandomForest(Array(\"SexuponOutcome\", \"AgeuponOutcome\", \"AnimalType\"))"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1462837624359_588471102","id":"20160509-194704_1727601298","dateCreated":"May 9, 2016 7:47:04 PM","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1527","dateUpdated":"May 9, 2016 9:29:41 PM","dateFinished":"May 9, 2016 9:30:43 PM","dateStarted":"May 9, 2016 9:30:08 PM","errorMessage":"","text":"// Strangly, the RandomForest performs worse when I have three features. It does better when there are two features.\n// Maybe I shouldn't have made the RandomForest, and instead worked on improving the performance of the decision tree. That's what I'll do now,\n// but first, I want to do a PCA to see how much each feature contributes to the first and second principal components.\nval pipeline = makePreprocessingPipeline(trainingDfWithDaysCol.columns)\nval preprocessedData = pipeline.fit(trainingDfWithDaysCol).transform(trainingDfWithDaysCol)\n\nimport org.apache.spark.ml.feature.PCA\nval pca = new PCA()\n    .setK(2)\n    .setInputCol(\"features\")\n    .setOutputCol(\"principalComponents\")\nval pcaModel = pca.fit(preprocessedData)\nval numRows = pcaModel.pc.numRows\nval (pc1, pc2) = pcaModel.pc.toArray.splitAt(numRows)\n\nval featuresField = preprocessedData.schema.find(_.name == \"features\").head\nimport org.apache.spark.ml.attribute.AttributeGroup\nval idxNameTypes = AttributeGroup.fromStructField(featuresField).attributes.get.map { attr =>\n    (attr.index.get, attr.name.get, attr.attrType)\n}\n\nprintln(idxNameTypes.mkString(\", \"))\n\ndef mostImportantComponents(pc: Array[Double]): Array[String] = {\n    (pc zip idxNameTypes)\n        .map { case (pcVal, (idx, name, attrType)) =>\n            (pcVal, idx, name)\n        }\n        .sortWith(_._1.abs <= _._1.abs)\n        .map { case (pcVal, idx, name) => name}\n}\n\nprintln(mostImportantComponents(pc1).mkString(\", \"))\nprintln(mostImportantComponents(pc2).mkString(\", \"))"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1462837674192_1456757424","id":"20160509-194754_1353466315","dateCreated":"May 9, 2016 7:47:54 PM","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1609","dateUpdated":"May 9, 2016 9:29:41 PM","dateFinished":"May 9, 2016 9:30:48 PM","dateStarted":"May 9, 2016 9:30:33 PM","errorMessage":"","text":"// Looking at the last two lines, it seems that the three most important dimensions of PCA are:\n// AnimalType, SexuponOutcome, and AgeuponOutcome. Color and breed follow these top three. Name and DateTime are the least important.\n// This makes sense. Let me try and do the decision trees again with these features.\nfullDecisionTree(Array(\"AnimalType\"))"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1462842918675_-1848651917","id":"20160509-211518_1777438303","dateCreated":"May 9, 2016 9:15:18 PM","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1983","dateUpdated":"May 9, 2016 9:29:41 PM","dateFinished":"May 9, 2016 9:30:54 PM","dateStarted":"May 9, 2016 9:30:43 PM","errorMessage":"","text":"// Using just animal type yields a super simple and pretty stupid decision tree. So, I'll add another feature.\nfullDecisionTree(Array(\"AnimalType\", \"SexuponOutcome\"))"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1462843637559_-1441020062","id":"20160509-212717_1285069553","dateCreated":"May 9, 2016 9:27:17 PM","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2135","dateUpdated":"May 9, 2016 9:29:41 PM","dateFinished":"May 9, 2016 9:31:01 PM","dateStarted":"May 9, 2016 9:30:49 PM","errorMessage":"","text":"// That's a little better.\nfullDecisionTree(Array(\"AnimalType\", \"SexuponOutcome\", \"AgeuponOutcome\"))"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1462843664062_715558819","id":"20160509-212744_1221789086","dateCreated":"May 9, 2016 9:27:44 PM","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2207","dateUpdated":"May 9, 2016 9:29:41 PM","dateFinished":"May 9, 2016 9:31:10 PM","dateStarted":"May 9, 2016 9:30:54 PM","errorMessage":"","text":"// This is much better. Now let me add breed and color.\nfullDecisionTree(Array(\"AnimalType\", \"SexuponOutcome\", \"AgeuponOutcome\", \"Breed\", \"Color\"))"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1462843696845_-136413163","id":"20160509-212816_201975863","dateCreated":"May 9, 2016 9:28:16 PM","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2279","dateUpdated":"May 9, 2016 9:33:08 PM","dateFinished":"May 9, 2016 9:33:08 PM","dateStarted":"May 9, 2016 9:33:08 PM","errorMessage":"","text":"// Adding the breed and color doesn't help much. I'm going to use "},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1462843988534_-346102958","id":"20160509-213308_333622003","dateCreated":"May 9, 2016 9:33:08 PM","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2500","errorMessage":""}],"name":"AnimalShelter","id":"2BKT31PE1","angularObjects":{"2BHWR53RN:shared_process":[],"2BK5DDM4J:shared_process":[],"2BKWKEWDS:shared_process":[],"2BHCSHN44:shared_process":[],"2BKC81PSD:shared_process":[],"2BJZWNTCG:shared_process":[],"2BKVUM25G:shared_process":[],"2BH3AMZ8Z:shared_process":[],"2BM7Q8B5Q:shared_process":[],"2BH6XH8E1:shared_process":[],"2BJKFFD9Z:shared_process":[],"2BHRJQXH7:shared_process":[],"2BHDNTQKH:shared_process":[],"2BJQAPGJX:shared_process":[],"2BJHY91FX:shared_process":[],"2BKPJ3DTK:shared_process":[],"2BHTSEV87:shared_process":[],"2BMR1CQKS:shared_process":[]},"config":{"looknfeel":"default"},"info":{}}